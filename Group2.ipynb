{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    #List of List for each layer along with their information\n",
    "    #Shape: [ [layerType,activationF(),neuronCount],[layerType,activationF(),neuronCount]]\n",
    "    layer_info = None\n",
    "    #List of weight matrices of each layer\n",
    "    #Shape: dxk (where d is no of input)(k is no of output)\n",
    "    w_l_ij= None\n",
    "    #List of bias vector of each layer\n",
    "    #Shape: 1xk (where k is no of output/neuron)\n",
    "    b_l_j = None\n",
    "    #List of output of each neuron\n",
    "    #Shape: 1xd (where d is dimension)\n",
    "    a_l_ij = None\n",
    "    #List of d(a_l+1)/da_l (where a_l+1 is output of layer)(a_l is input of layer)\n",
    "    #Shape: 1xd (same shape as input of layer)\n",
    "    delta_l_ij = None\n",
    "    #List of matrices of dl/dw for each layer\n",
    "    dl_db = None\n",
    "    dl_dw = None\n",
    "    #Dictionary for differentiation function for a input function. diff[f()] => df()\n",
    "    diff = {}\n",
    "    input_featureCount = None\n",
    "    input_count = None\n",
    "    def __init__(self,xShape,rseed = 10,listLayer= None):\n",
    "        np.random.seed(rseed)\n",
    "        inputCount,inputDim = xShape\n",
    "        self.input_featureCount = inputDim\n",
    "        self.input_count = inputCount\n",
    "        self.layer_info = [['None','None',self.input_featureCount]]\n",
    "        self.w_l_ij = ['None']\n",
    "        self.b_l_j = ['None']\n",
    "        self.optimizer = None\n",
    "        self.diff[ANN.activation_linear] = ANN.diff_activation_linear\n",
    "        self.diff[ANN.activation_sigmoid] = ANN.diff_activation_sigmoid\n",
    "        self.diff[ANN.activation_tanh] = ANN.diff_activation_tanh\n",
    "\n",
    "    def compile(self,optimizer,lossFunction):\n",
    "        self.optimizer = optimizer\n",
    "        self.layer_info.append(['Loss',lossFunction])\n",
    "\n",
    "    def __str__(self):\n",
    "        return\n",
    "\n",
    "    def addLayers_Dense(self,neuronCount,activationFunction):\n",
    "            self.layer_info.append(['Dense',activationFunction,neuronCount])\n",
    "            w = np.random.rand(self.layer_info[-2][2],neuronCount)\n",
    "            b = np.random.rand(1,neuronCount)\n",
    "            self.w_l_ij.append(w)\n",
    "            self.b_l_j.append(b)\n",
    "\n",
    "    def addLayers_softmax(self):\n",
    "        self.layer_info.append(['Softmax',self.layer_info[-1][2]])\n",
    "\n",
    "    def forward_layer_dense(self,layerNo):\n",
    "        if layerNo < 1:\n",
    "            print(\"ERRRROR\")\n",
    "        \n",
    "        #Output = activationFunction(a_l-1 @ w_l + b_l)\n",
    "        output = self.layer_info[layerNo][1](self.a_l_ij[layerNo-1] @ self.w_l_ij[layerNo] + self.b_l_j[layerNo]) \n",
    "        self.a_l_ij.append(output)\n",
    "    \n",
    "    def backward_layer_dense(self,layerNo):\n",
    "        #at each layer we need to calculate da_l/da_l-1, and da_l/dw_l\n",
    "\n",
    "        #Shape: dxk\n",
    "        da_1da = self.w_l_ij[layerNo].T\n",
    "        #Shape: dxk\n",
    "        da_l1_dw_l = np.copy(self.a_l_ij[layerNo-1]).T\n",
    "        for i in range(self.w_l_ij[layerNo].shape[1]-1):\n",
    "            da_l1_dw_l = np.hstack((da_l1_dw_l,self.a_l_ij[layerNo-1].T))\n",
    "\n",
    "        #multiply both by dL/da_l to get dL/da_l-1 and dL/dw_l\n",
    "        self.delta_l_ij[layerNo] = self.delta_l_ij[layerNo+1] @ da_1da\n",
    "        #print(da_l1_dw_l,\"\\n\", self.delta_l_ij[layerNo+1])\n",
    "        temp = np.zeros(self.w_l_ij[layerNo].shape)\n",
    "        for i in range(self.w_l_ij[layerNo].shape[0]):\n",
    "            for j in range(self.w_l_ij[layerNo].shape[1]):\n",
    "                temp[i][j] = da_l1_dw_l[i][j] * self.delta_l_ij[layerNo][0][j]\n",
    "\n",
    "        self.dl_dw.append(temp)\n",
    "        self.dl_db.append(self.delta_l_ij[layerNo+1])\n",
    "    \n",
    "    def forward_layer_softmax(self,layerNo):\n",
    "        if layerNo < 1: \n",
    "            print(\"ERRORRR\")\n",
    "        denom = np.sum( np.exp(self.a_l_ij[layerNo-1]) )\n",
    "        output = np.exp(self.a_l_ij[layerNo]) / denom\n",
    "        self.a_l_ij.append(output)\n",
    "\n",
    "    def backward_layer_softmax(self,layerNo):\n",
    "        pass\n",
    "\n",
    "    #Create cross entropy loss \n",
    "    def forward_layer_loss(self,layerNo,y):\n",
    "        y_hat  =  self.a_l_ij[-1]\n",
    "        lossFunction = self.layer_info[layerNo][1]\n",
    "        return lossFunction(y_hat,y)\n",
    "\n",
    "    def forwardProp(self,x,y):\n",
    "        self.a_l_ij = []\n",
    "        self.a_l_ij.append(x)\n",
    "        for layerNo in range(len(self.layer_info)):\n",
    "            if self.layer_info[layerNo][0] == 'Dense':\n",
    "                self.forward_layer_dense(layerNo)\n",
    "            if self.layer_info[layerNo][0] == 'Softmax':\n",
    "                self.forward_layer_softmax(layerNo)\n",
    "            if self.layer_info[layerNo][0] == 'Loss':\n",
    "                cost = self.forward_layer_loss(layerNo,y)\n",
    "                return cost\n",
    "            \n",
    "    #Handle softmax backpropogation\n",
    "    def backwardProp(self,y):\n",
    "        self.dl_dw = []\n",
    "        self.dl_db = []\n",
    "        lastIndex = len(self.layer_info) - 2\n",
    "        self.delta_l_ij = [i for i in range(0,lastIndex+1)]\n",
    "        \n",
    "        #MSE LOSS And Dense\n",
    "        self.delta_l_ij[lastIndex] = 2 * ( (self.a_l_ij[lastIndex] - y) @ self.w_l_ij[lastIndex].T) \n",
    "        self.dl_dw.append(2*(self.a_l_ij[lastIndex-1].T @ self.a_l_ij[lastIndex] - y))\n",
    "        self.dl_db.append(2*(self.a_l_ij[lastIndex] - y))\n",
    "\n",
    "        #BackPropogation\n",
    "        for layerNo in range(lastIndex-1,0,-1):\n",
    "            if self.layer_info[layerNo][0] == 'Dense':\n",
    "                self.backward_layer_dense(layerNo)\n",
    "            if self.layer_info[layerNo][0] == 'Softmax':\n",
    "                self.backward_layer_softmax(layerNo)\n",
    "                \n",
    "        self.dl_dw.append(['None'])\n",
    "        self.dl_db.append(['None'])\n",
    "        self.dl_dw.reverse()\n",
    "        self.dl_db.reverse()\n",
    "\n",
    "\n",
    "            \n",
    "    def fit(self,x,y):\n",
    "        #Hyperparameters if required: \n",
    "        epoch = 100\n",
    "        eta = 1e-6\n",
    "        self.optimizer(self,x,y,eta,epoch)\n",
    "\n",
    "    @staticmethod    \n",
    "    def optimizer_gradientDescent(Obj,x,y,eta,epoch):\n",
    "        for i in  range(epoch):\n",
    "            errorSum = 0\n",
    "            print(\"Epoch:\",i,end=\"\\t[\")\n",
    "            for j in range(1):\n",
    "                #x and y both are 2d matrix\n",
    "                x_1xd = x[j:j+1]\n",
    "                y_1xk = y[j:j+1]\n",
    "                errorSum += Obj.forwardProp(x_1xd,y_1xk)\n",
    "                Obj.backwardProp(y_1xk)\n",
    "                for index in range(1,len(Obj.w_l_ij)):\n",
    "                    Obj.w_l_ij[index] = Obj.w_l_ij[index] - (eta * Obj.dl_dw[index])\n",
    "                    Obj.b_l_j[index] = Obj.b_l_j[index] - (eta * Obj.dl_db[index] )\n",
    "            print(\"]\", end='\\t')\n",
    "            print(\"Error:\",errorSum)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_MSE(P,y):\n",
    "        return np.sum((P - y)**2)   \n",
    "      \n",
    "    @staticmethod\n",
    "    def activation_linear(z):\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def diff_activation_linear(x):\n",
    "        return np.ones(x.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def activation_sigmoid(z):    \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod    \n",
    "    def diff_activation_sigmoid(a):\n",
    "        return a * (1- a)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_tanh(z):\n",
    "        sinh = np.exp(z) - np.exp(-z)\n",
    "        cosh = np.exp(z) + np.exp(-z)\n",
    "        return sinh / cosh\n",
    "    \n",
    "    @staticmethod\n",
    "    def diff_activation_tanh(z):\n",
    "        return 1 - (z * z)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_relu(z):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.datasets as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "          37.88      , -122.23      ],\n",
       "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "          37.86      , -122.22      ],\n",
       "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "          37.85      , -122.24      ],\n",
       "       ...,\n",
       "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "          39.43      , -121.22      ],\n",
       "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "          39.43      , -121.32      ],\n",
       "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "          39.37      , -121.24      ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictt = sk.fetch_california_housing()\n",
    "x = dictt.data\n",
    "y = np.array(dictt.target)\n",
    "y = y.T\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import Normalizer\n",
    "# transformer = Normalizer().fit(x)\n",
    "# x = transformer.transform(x)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(x.shape,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.addLayers_Dense(4,ANN.activation_linear)\n",
    "model.addLayers_Dense(3,ANN.activation_linear)\n",
    "model.addLayers_Dense(1,ANN.activation_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(ANN.optimizer_gradientDescent,ANN.loss_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['None', 'None', 8],\n",
       " ['Dense', <function __main__.ANN.activation_linear(z)>, 4],\n",
       " ['Dense', <function __main__.ANN.activation_linear(z)>, 3],\n",
       " ['Dense', <function __main__.ANN.activation_linear(z)>, 1],\n",
       " ['Loss', <function __main__.ANN.loss_MSE(P, y)>]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "w_old = copy.deepcopy(model.w_l_ij)\n",
    "b_old = copy.deepcopy(model.b_l_j)\n",
    "dldw = copy.deepcopy(model.dl_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t[]\tError: 128384.96048739352\n",
      "Epoch: 1\t[]\tError: 24495.659407641015\n",
      "Epoch: 2\t[]\tError: 720.1462495051715\n",
      "Epoch: 3\t[]\tError: 79.05511746785547\n",
      "Epoch: 4\t[]\tError: 9.109561952916417\n",
      "Epoch: 5\t[]\tError: 1.0740339161167474\n",
      "Epoch: 6\t[]\tError: 0.13237708407991283\n",
      "Epoch: 7\t[]\tError: 0.018317109942635596\n",
      "Epoch: 8\t[]\tError: 0.0033282021430480263\n",
      "Epoch: 9\t[]\tError: 0.0009799253916001182\n",
      "Epoch: 10\t[]\tError: 0.0004989355779448052\n",
      "Epoch: 11\t[]\tError: 0.00037203543614244437\n",
      "Epoch: 12\t[]\tError: 0.0003330526339760441\n",
      "Epoch: 13\t[]\tError: 0.00032018863137534285\n",
      "Epoch: 14\t[]\tError: 0.00031575940692064744\n",
      "Epoch: 15\t[]\tError: 0.0003141427186268393\n",
      "Epoch: 16\t[]\tError: 0.0003134745899757143\n",
      "Epoch: 17\t[]\tError: 0.0003131277488266941\n",
      "Epoch: 18\t[]\tError: 0.00031289004615144824\n",
      "Epoch: 19\t[]\tError: 0.0003126896214397846\n",
      "Epoch: 20\t[]\tError: 0.0003125021251008625\n",
      "Epoch: 21\t[]\tError: 0.00031231930659095166\n",
      "Epoch: 22\t[]\tError: 0.0003121383705452513\n",
      "Epoch: 23\t[]\tError: 0.0003119583697446308\n",
      "Epoch: 24\t[]\tError: 0.00031177898295627765\n",
      "Epoch: 25\t[]\tError: 0.00031160010091318464\n",
      "Epoch: 26\t[]\tError: 0.00031142168611226676\n",
      "Epoch: 27\t[]\tError: 0.00031124372533201255\n",
      "Epoch: 28\t[]\tError: 0.00031106621356976336\n",
      "Epoch: 29\t[]\tError: 0.00031088914860660024\n",
      "Epoch: 30\t[]\tError: 0.0003107125291698995\n",
      "Epoch: 31\t[]\tError: 0.00031053635430179085\n",
      "Epoch: 32\t[]\tError: 0.0003103606231557969\n",
      "Epoch: 33\t[]\tError: 0.0003101853349207447\n",
      "Epoch: 34\t[]\tError: 0.0003100104888073908\n",
      "Epoch: 35\t[]\tError: 0.00030983608403231753\n",
      "Epoch: 36\t[]\tError: 0.0003096621198077957\n",
      "Epoch: 37\t[]\tError: 0.0003094885953546649\n",
      "Epoch: 38\t[]\tError: 0.0003093155098966655\n",
      "Epoch: 39\t[]\tError: 0.0003091428626641188\n",
      "Epoch: 40\t[]\tError: 0.0003089706528772436\n",
      "Epoch: 41\t[]\tError: 0.0003087988797758545\n",
      "Epoch: 42\t[]\tError: 0.00030862754258431993\n",
      "Epoch: 43\t[]\tError: 0.0003084566405424686\n",
      "Epoch: 44\t[]\tError: 0.0003082861728876951\n",
      "Epoch: 45\t[]\tError: 0.0003081161388610118\n",
      "Epoch: 46\t[]\tError: 0.0003079465377032706\n",
      "Epoch: 47\t[]\tError: 0.00030777736866136355\n",
      "Epoch: 48\t[]\tError: 0.00030760863098263703\n",
      "Epoch: 49\t[]\tError: 0.0003074403239151702\n",
      "Epoch: 50\t[]\tError: 0.0003072724467138449\n",
      "Epoch: 51\t[]\tError: 0.00030710499863202337\n",
      "Epoch: 52\t[]\tError: 0.00030693797892537784\n",
      "Epoch: 53\t[]\tError: 0.0003067713868550903\n",
      "Epoch: 54\t[]\tError: 0.0003066052216853868\n",
      "Epoch: 55\t[]\tError: 0.0003064394826750746\n",
      "Epoch: 56\t[]\tError: 0.00030627416909396224\n",
      "Epoch: 57\t[]\tError: 0.0003061092802104741\n",
      "Epoch: 58\t[]\tError: 0.00030594481529419944\n",
      "Epoch: 59\t[]\tError: 0.0003057807736212631\n",
      "Epoch: 60\t[]\tError: 0.00030561715446614864\n",
      "Epoch: 61\t[]\tError: 0.0003054539571038418\n",
      "Epoch: 62\t[]\tError: 0.00030529118081994676\n",
      "Epoch: 63\t[]\tError: 0.00030512882489447374\n",
      "Epoch: 64\t[]\tError: 0.00030496688861161706\n",
      "Epoch: 65\t[]\tError: 0.0003048053712613923\n",
      "Epoch: 66\t[]\tError: 0.00030464427213076\n",
      "Epoch: 67\t[]\tError: 0.00030448359051044797\n",
      "Epoch: 68\t[]\tError: 0.0003043233256994388\n",
      "Epoch: 69\t[]\tError: 0.00030416347698884776\n",
      "Epoch: 70\t[]\tError: 0.00030400404367943195\n",
      "Epoch: 71\t[]\tError: 0.00030384502507414476\n",
      "Epoch: 72\t[]\tError: 0.0003036864204713838\n",
      "Epoch: 73\t[]\tError: 0.0003035282291800983\n",
      "Epoch: 74\t[]\tError: 0.00030337045050566583\n",
      "Epoch: 75\t[]\tError: 0.00030321308375759796\n",
      "Epoch: 76\t[]\tError: 0.0003030561282520694\n",
      "Epoch: 77\t[]\tError: 0.00030289958329768747\n",
      "Epoch: 78\t[]\tError: 0.00030274344821336514\n",
      "Epoch: 79\t[]\tError: 0.0003025877223175539\n",
      "Epoch: 80\t[]\tError: 0.0003024324049306219\n",
      "Epoch: 81\t[]\tError: 0.0003022774953764873\n",
      "Epoch: 82\t[]\tError: 0.00030212299297853695\n",
      "Epoch: 83\t[]\tError: 0.00030196889706438014\n",
      "Epoch: 84\t[]\tError: 0.00030181520696260134\n",
      "Epoch: 85\t[]\tError: 0.00030166192201047155\n",
      "Epoch: 86\t[]\tError: 0.00030150904153345505\n",
      "Epoch: 87\t[]\tError: 0.0003013565648707257\n",
      "Epoch: 88\t[]\tError: 0.00030120449136306337\n",
      "Epoch: 89\t[]\tError: 0.00030105282034779637\n",
      "Epoch: 90\t[]\tError: 0.0003009015511671552\n",
      "Epoch: 91\t[]\tError: 0.0003007506831666945\n",
      "Epoch: 92\t[]\tError: 0.0003006002156891586\n",
      "Epoch: 93\t[]\tError: 0.0003004501480892022\n",
      "Epoch: 94\t[]\tError: 0.0003003004797133351\n",
      "Epoch: 95\t[]\tError: 0.0003001512099159339\n",
      "Epoch: 96\t[]\tError: 0.00030000233805240124\n",
      "Epoch: 97\t[]\tError: 0.00029985386347765623\n",
      "Epoch: 98\t[]\tError: 0.00029970578555293026\n",
      "Epoch: 99\t[]\tError: 0.00029955810363469014\n"
     ]
    }
   ],
   "source": [
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[   8.3252    ,   41.        ,    6.98412698,    1.02380952,\n",
       "          322.        ,    2.55555556,   37.88      , -122.23      ]]),\n",
       " array([[-121.13504572,  154.22447139,  -33.54439386,  -42.03656395]]),\n",
       " array([[-31.32529087,   4.57665221,  16.27493424]]),\n",
       " array([[4.50869225]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a_l_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " array([[0.5881308 , 0.89771373, 0.89153073, 0.81583748],\n",
       "        [0.03588959, 0.69175758, 0.37868094, 0.51851095],\n",
       "        [0.65795147, 0.19385022, 0.2723164 , 0.71860593],\n",
       "        [0.78300361, 0.85032764, 0.77524489, 0.03666431],\n",
       "        [0.11669374, 0.7512807 , 0.23921822, 0.25480601],\n",
       "        [0.85762553, 0.94977903, 0.56168686, 0.17878052],\n",
       "        [0.77025193, 0.49238104, 0.63125307, 0.83949792],\n",
       "        [0.4610394 , 0.49794007, 0.67941112, 0.65078591]]),\n",
       " array([[0.32920641, 0.51064106, 0.26362883],\n",
       "        [0.31051155, 0.62685344, 0.55744981],\n",
       "        [0.31857956, 0.39484322, 0.25797459],\n",
       "        [0.58224112, 0.16162871, 0.59813382]]),\n",
       " array([[0.40864343],\n",
       "        [0.7786879 ],\n",
       "        [0.80397057]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " array([[0.26848432, 0.06681538, 0.77116834, 0.48061691]]),\n",
       " array([[0.82564995, 0.15608496, 0.73398557]]),\n",
       " array([[0.78575175]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.b_l_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " array([[0.26879524, 0.06732467, 0.77144514, 0.48098413]]),\n",
       " array([[0.82582358, 0.15639172, 0.73430052]]),\n",
       " array([[0.78607144]])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['None'],\n",
       " array([[ -0.48378502,  -0.24081496,  -0.27184988,  -0.36823632],\n",
       "        [ -2.38254764,  -1.18596712,  -1.33880811,  -1.81349265],\n",
       "        [ -0.40585403,  -0.20202305,  -0.22805868,  -0.30891861],\n",
       "        [ -0.05949451,  -0.02961474,  -0.03343133,  -0.04528466],\n",
       "        [-18.71171563,  -9.3141808 , -10.51454171, -14.24255203],\n",
       "        [ -0.14850568,  -0.07392207,  -0.08344874,  -0.11303613],\n",
       "        [ -2.20124158,  -1.09571792,  -1.23692807,  -1.67549028],\n",
       "        [  7.10289752,   3.53562832,   3.99128085,   5.40641967]]),\n",
       " array([[ 2.03547526,  2.69771016,  1.79750515],\n",
       "        [-2.59148865, -3.4346206 , -2.28851427],\n",
       "        [ 0.56365838,  0.7470427 ,  0.49776033],\n",
       "        [ 0.70635534,  0.93616562,  0.6237744 ]]),\n",
       " array([[-291.52419252],\n",
       "        [  32.21743272],\n",
       "        [ 137.70533983]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dl_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dldw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.delta_l_ij = [i for i in range(0,lastIndex+1)]\n",
    "# self.delta_l_ij[lastIndex] = (2 * (self.a_l_ij[lastIndex] - y).T * self.diff[self.layer_info[lastIndex][1]](self.a_l_ij[lastIndex]))\n",
    "# self.dl_dw.append(self.a_l_ij[lastIndex-1].T @ self.delta_l_ij[lastIndex])\n",
    "# for layerNo in range(lastIndex-1,0,-1):\n",
    "#     self.delta_l_ij[layerNo] = (self.w_l_ij[layerNo+1] @self.delta_l_ij[layerNo+1].T).T * self.diff[self.layer_info[layerNo][1]](self.a_l_ij[layerNo])\n",
    "#     self.dl_dw.append(self.a_l_ij[layerNo-1].T @ self.delta_l_ij[layerNo])\n",
    "# self.dl_dw.append(['None'])\n",
    "# self.dl_dw.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # self.delta_l_ij = [i for i in range(0,lastIndex+1)]\n",
    "        # self.delta_l_ij[lastIndex] = (2 * (self.a_l_ij[lastIndex] - y).T * self.diff[self.layer_info[lastIndex][1]](self.a_l_ij[lastIndex]))\n",
    "        # self.dl_dw.append(self.a_l_ij[lastIndex-1].T @ self.delta_l_ij[lastIndex])\n",
    "        # self.dl_db.append(self.a_l_ij[lastIndex])\n",
    "        # for layerNo in range(lastIndex-1,0,-1):\n",
    "        #     self.delta_l_ij[layerNo] = (self.w_l_ij[layerNo+1] @self.delta_l_ij[layerNo+1].T).T * self.diff[self.layer_info[layerNo][1]](self.a_l_ij[layerNo])\n",
    "        #     self.dl_dw.append(self.a_l_ij[layerNo-1].T @ self.delta_l_ij[layerNo])\n",
    "        #     self.dl_db.append(self.delta_l_ij[layerNo])\n",
    "        # self.dl_dw.append(['None'])\n",
    "        # self.dl_db.append(['None'])\n",
    "        # self.dl_dw.reverse()\n",
    "        # self.dl_dw.reverse()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [2, 2],\n",
       "       [3, 3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2,3]])\n",
    "x_ = np.hstack((x.T,x.T))\n",
    "x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Function (Backpropogation)\n",
    "MSE -> Generalise for any loss\n",
    "Softmax backProp\n",
    "Stopping condition stochastic gd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_331",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
