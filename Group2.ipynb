{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    #List of List for each layer along with their information\n",
    "    #Shape: [ [layerType,activationF(),neuronCount],[layerType,activationF(),neuronCount]]\n",
    "    layer_info = None\n",
    "    #List of weight matrices of each layer\n",
    "    #Shape: dxk (where d is no of input)(k is no of output)\n",
    "    w_l_ij= None\n",
    "    #List of bias vector of each layer\n",
    "    #Shape: 1xk (where k is no of output/neuron)\n",
    "    b_l_j = None\n",
    "    #List of output of each neuron\n",
    "    #Shape: 1xd (where d is dimension)\n",
    "    a_l_ij = None\n",
    "    #List of d(a_l+1)/da_l (where a_l+1 is output of layer)(a_l is input of layer)\n",
    "    #Shape: 1xd (same shape as input of layer)\n",
    "    delta_l_ij = None\n",
    "    #List of matrices of dl/dw for each layer\n",
    "    dl_db = None\n",
    "    dl_dw = None\n",
    "    #Dictionary for differentiation function for a input function. diff[f()] => df()\n",
    "    diff = {}\n",
    "    input_featureCount = None\n",
    "    input_count = None\n",
    "    def __init__(self,xShape,rseed = 10,listLayer= None):\n",
    "        np.random.seed(rseed)\n",
    "        inputCount,inputDim = xShape\n",
    "        self.input_featureCount = inputDim\n",
    "        self.input_count = inputCount\n",
    "        self.layer_info = [['None','None',self.input_featureCount]]\n",
    "        self.w_l_ij = ['None']\n",
    "        self.b_l_j = ['None']\n",
    "        self.optimizer = None\n",
    "        self.diff[ANN.activation_linear] = ANN.diff_activation_linear\n",
    "        self.diff[ANN.activation_sigmoid] = ANN.diff_activation_sigmoid\n",
    "        self.diff[ANN.activation_tanh] = ANN.diff_activation_tanh\n",
    "\n",
    "    def compile(self,optimizer,lossFunction):\n",
    "        self.optimizer = optimizer\n",
    "        self.layer_info.append(['Loss',lossFunction])\n",
    "\n",
    "    def __str__(self):\n",
    "        return\n",
    "\n",
    "    def addLayers_Dense(self,neuronCount,activationFunction):\n",
    "            self.layer_info.append(['Dense',activationFunction,neuronCount])\n",
    "            w = np.random.rand(self.layer_info[-2][2],neuronCount)\n",
    "            b = np.random.rand(1,neuronCount)\n",
    "            self.w_l_ij.append(w)\n",
    "            self.b_l_j.append(b)\n",
    "\n",
    "    def addLayers_softmax(self):\n",
    "        self.layer_info.append(['Softmax',self.layer_info[-1][2]])\n",
    "        self.w_l_ij.append('None')\n",
    "        self.b_l_j.append('None')\n",
    "\n",
    "    def forward_layer_dense(self,layerNo):\n",
    "        if layerNo < 1:\n",
    "            print(\"ERRRROR\")\n",
    "        \n",
    "        #Output = activationFunction(a_l-1 @ w_l + b_l)\n",
    "        output = self.layer_info[layerNo][1](self.a_l_ij[layerNo-1] @ self.w_l_ij[layerNo] + self.b_l_j[layerNo]) \n",
    "        self.a_l_ij.append(output)\n",
    "    \n",
    "    def backward_layer_dense(self,layerNo):\n",
    "        #at each layer we need to calculate da_l/da_l-1, and da_l/dw_l\n",
    "        #Shape: dxk\n",
    "        da_1da = self.diff[self.layer_info[layerNo][1]](self.a_l_ij[layerNo]).T @ self.w_l_ij[layerNo].T\n",
    "        #Shape: dxk\n",
    "        da_l1_dw_l = np.zeros(self.w_l_ij[layerNo].shape)\n",
    "        for i in range(da_l1_dw_l.shape[0]):\n",
    "            for j in range(da_l1_dw_l.shape[1]):\n",
    "                da_l1_dw_l[i][j] = self.diff[self.layer_info[layerNo][1]](self.a_l_ij[layerNo][0][j],'SINGLE') * self.a_l_ij[layerNo-1][0][i]\n",
    "\n",
    "        #multiply both by dL/da_l to get dL/da_l-1 and dL/dw_l\n",
    "        self.delta_l_ij[layerNo] = self.delta_l_ij[layerNo+1] @ da_1da\n",
    "\n",
    "        #print(\"AAA:\",da_l1_dw_l,\"\\n\", self.delta_l_ij[layerNo+1])\n",
    "        temp = np.zeros(self.w_l_ij[layerNo].shape)\n",
    "        for i in range(temp.shape[0]):\n",
    "            for j in range(temp.shape[1]):\n",
    "                temp[i][j] = da_l1_dw_l[i][j] * self.delta_l_ij[layerNo+1][0][j]\n",
    "\n",
    "        self.dl_dw.append(temp)\n",
    "        self.dl_db.append(self.delta_l_ij[layerNo+1])\n",
    "    \n",
    "    def forward_layer_softmax(self,layerNo):\n",
    "        if layerNo < 1: \n",
    "            print(\"ERRORRR\")\n",
    "        denom = np.sum( np.exp(self.a_l_ij[layerNo-1]) )\n",
    "        output = np.exp(self.a_l_ij[layerNo-1]) / denom\n",
    "        self.a_l_ij.append(output)\n",
    "\n",
    "    def backward_layer_softmax(self,layerNo):\n",
    "        pass\n",
    "\n",
    "    #Create cross entropy loss \n",
    "    def forward_layer_loss(self,layerNo,y):\n",
    "        y_hat  =  self.a_l_ij[-1]\n",
    "        lossFunction = self.layer_info[layerNo][1]\n",
    "        return lossFunction(y_hat,y)\n",
    "\n",
    "    def forwardProp(self,x,y):\n",
    "        self.a_l_ij = []\n",
    "        self.a_l_ij.append(x)\n",
    "        for layerNo in range(len(self.layer_info)):\n",
    "            if self.layer_info[layerNo][0] == 'Dense':\n",
    "                self.forward_layer_dense(layerNo)\n",
    "            if self.layer_info[layerNo][0] == 'Softmax':\n",
    "                self.forward_layer_softmax(layerNo)\n",
    "            if self.layer_info[layerNo][0] == 'Loss':\n",
    "                cost = self.forward_layer_loss(layerNo,y)\n",
    "                return cost\n",
    "            \n",
    "    #Handle softmax backpropogation\n",
    "    def backwardProp(self,y):\n",
    "        self.dl_dw = []\n",
    "        self.dl_db = []\n",
    "        lastIndex = len(self.layer_info) - 2\n",
    "        self.delta_l_ij = [i for i in range(0,lastIndex+1)]\n",
    "        \n",
    "        #MSE LOSS And Dense\n",
    "        self.delta_l_ij[lastIndex] = 2 * ( (self.a_l_ij[lastIndex] - y) @ self.w_l_ij[lastIndex].T) \n",
    "        self.dl_dw.append(2*(self.a_l_ij[lastIndex-1].T @ self.a_l_ij[lastIndex] - y))\n",
    "        self.dl_db.append(2*(self.a_l_ij[lastIndex] - y))\n",
    "\n",
    "        #BackPropogation\n",
    "        for layerNo in range(lastIndex-1,0,-1):\n",
    "            if self.layer_info[layerNo][0] == 'Dense':\n",
    "                self.backward_layer_dense(layerNo)\n",
    "            if self.layer_info[layerNo][0] == 'Softmax':\n",
    "                self.backward_layer_softmax(layerNo)\n",
    "                \n",
    "        self.dl_dw.append(['None'])\n",
    "        self.dl_db.append(['None'])\n",
    "        self.dl_dw.reverse()\n",
    "        self.dl_db.reverse()\n",
    "\n",
    "\n",
    "            \n",
    "    def fit(self,x,y):\n",
    "        #Hyperparameters if required: \n",
    "        epoch = 100\n",
    "        eta = 4e-5\n",
    "        self.optimizer(self,x,y,eta,epoch)\n",
    "\n",
    "    @staticmethod    \n",
    "    def optimizer_gradientDescent(Obj,x,y,eta,epoch):\n",
    "        for i in  range(epoch):\n",
    "            errorSum = 0\n",
    "            print(\"Epoch:\",i,end=\"\\t\")\n",
    "            for j in range(1):\n",
    "                #x and y both are 2d matrix\n",
    "                x_1xd = x[j:j+1]\n",
    "                y_1xk = y[j:j+1]\n",
    "                errorSum += Obj.forwardProp(x_1xd,y_1xk)\n",
    "                Obj.backwardProp(y_1xk)\n",
    "                for index in range(1,len(Obj.w_l_ij)):\n",
    "                    Obj.w_l_ij[index] = Obj.w_l_ij[index] - (eta * Obj.dl_dw[index])\n",
    "                    Obj.b_l_j[index] = Obj.b_l_j[index] - (eta * Obj.dl_db[index] )\n",
    "            print(\"Error:\",errorSum)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_MSE(P,y):\n",
    "        return np.sum((P - y)**2)   \n",
    "      \n",
    "    @staticmethod\n",
    "    def activation_linear(z):\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def diff_activation_linear(x,flg='None'):\n",
    "        if(flg != 'SINGLE'):\n",
    "            return np.identity(x.shape[1])\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def activation_sigmoid(z):    \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod    \n",
    "    def diff_activation_sigmoid(a,flg='None'):\n",
    "        if(flg !='SINGLE'):\n",
    "            return np.diag(a * (1-a))\n",
    "        else:\n",
    "            return a * (1- a)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_tanh(z):\n",
    "        sinh = np.exp(z) - np.exp(-z)\n",
    "        cosh = np.exp(z) + np.exp(-z)\n",
    "        return sinh / cosh\n",
    "    \n",
    "    @staticmethod\n",
    "    def diff_activation_tanh(z,flg='None'):\n",
    "        if(flg != 'SINGLE'):\n",
    "            return np.diag(1 - (z*z))\n",
    "        else:\n",
    "            return 1 - (z * z)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_relu(z):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.datasets as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "          37.88      , -122.23      ],\n",
       "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "          37.86      , -122.22      ],\n",
       "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "          37.85      , -122.24      ],\n",
       "       ...,\n",
       "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "          39.43      , -121.22      ],\n",
       "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "          39.43      , -121.32      ],\n",
       "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "          39.37      , -121.24      ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictt = sk.fetch_california_housing()\n",
    "x = dictt.data\n",
    "y = np.array(dictt.target)\n",
    "y = y.T\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0238481 ,  0.1174473 ,  0.02000651, ...,  0.00732056,\n",
       "         0.10850985, -0.3501362 ],\n",
       "       [ 0.00345241,  0.00873354,  0.00259434, ...,  0.00087745,\n",
       "         0.01574533, -0.05082923],\n",
       "       [ 0.01409202,  0.10097076,  0.01609345, ...,  0.00544128,\n",
       "         0.07349506, -0.23735895],\n",
       "       ...,\n",
       "       [ 0.00167455,  0.01674553,  0.00512762, ...,  0.00229082,\n",
       "         0.03883978, -0.11940547],\n",
       "       [ 0.00248251,  0.02393168,  0.00708579, ...,  0.00282289,\n",
       "         0.05242368, -0.16129955],\n",
       "       [ 0.00171478,  0.0114864 ,  0.00377236, ...,  0.00187873,\n",
       "         0.02826371, -0.08703817]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "transformer = Normalizer().fit(x)\n",
    "x = transformer.transform(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(x.shape,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.addLayers_Dense(9,ANN.activation_linear)\n",
    "model.addLayers_Dense(4,ANN.activation_linear)\n",
    "model.addLayers_Dense(1,ANN.activation_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(ANN.optimizer_gradientDescent,ANN.loss_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['None', 'None', 8],\n",
       " ['Dense', <function __main__.ANN.activation_linear(z)>, 9],\n",
       " ['Dense', <function __main__.ANN.activation_linear(z)>, 4],\n",
       " ['Dense', <function __main__.ANN.activation_linear(z)>, 1],\n",
       " ['Loss', <function __main__.ANN.loss_MSE(P, y)>]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "w_old = copy.deepcopy(model.w_l_ij)\n",
    "b_old = copy.deepcopy(model.b_l_j)\n",
    "dldw = copy.deepcopy(model.dl_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tError: 9.974453712895581\n",
      "Epoch: 1\tError: 9.65004927683404\n",
      "Epoch: 2\tError: 9.33541947232648\n",
      "Epoch: 3\tError: 9.030237327332655\n",
      "Epoch: 4\tError: 8.734189581401962\n",
      "Epoch: 5\tError: 8.44697598843659\n",
      "Epoch: 6\tError: 8.168308660987657\n",
      "Epoch: 7\tError: 7.897911453251741\n",
      "Epoch: 8\tError: 7.635519380152611\n",
      "Epoch: 9\tError: 7.380878070091701\n",
      "Epoch: 10\tError: 7.133743249133179\n",
      "Epoch: 11\tError: 6.893880254556057\n",
      "Epoch: 12\tError: 6.661063575859092\n",
      "Epoch: 13\tError: 6.435076421444371\n",
      "Epoch: 14\tError: 6.215710309334814\n",
      "Epoch: 15\tError: 6.002764680399225\n",
      "Epoch: 16\tError: 5.796046532667597\n",
      "Epoch: 17\tError: 5.59537007542005\n",
      "Epoch: 18\tError: 5.400556401825047\n",
      "Epoch: 19\tError: 5.211433178988075\n",
      "Epoch: 20\tError: 5.027834354350561\n",
      "Epoch: 21\tError: 4.849599877451541\n",
      "Epoch: 22\tError: 4.676575436131762\n",
      "Epoch: 23\tError: 4.50861220632183\n",
      "Epoch: 24\tError: 4.345566614613669\n",
      "Epoch: 25\tError: 4.187300112867521\n",
      "Epoch: 26\tError: 4.033678964156047\n",
      "Epoch: 27\tError: 3.884574039392673\n",
      "Epoch: 28\tError: 3.7398606240338177\n",
      "Epoch: 29\tError: 3.5994182342835943\n",
      "Epoch: 30\tError: 3.463130442266356\n",
      "Epoch: 31\tError: 3.330884709666278\n",
      "Epoch: 32\tError: 3.2025722293644443\n",
      "Epoch: 33\tError: 3.078087774633511\n",
      "Epoch: 34\tError: 2.957329555477126\n",
      "Epoch: 35\tError: 2.8401990817266625\n",
      "Epoch: 36\tError: 2.726601032531496\n",
      "Epoch: 37\tError: 2.6164431319012142\n",
      "Epoch: 38\tError: 2.5096360299785125\n",
      "Epoch: 39\tError: 2.406093189740884\n",
      "Epoch: 40\tError: 2.305730778847066\n",
      "Epoch: 41\tError: 2.2084675663609135\n",
      "Epoch: 42\tError: 2.114224824101092\n",
      "Epoch: 43\tError: 2.0229262323794623\n",
      "Epoch: 44\tError: 1.9344977899049107\n",
      "Epoch: 45\tError: 1.848867727641959\n",
      "Epoch: 46\tError: 1.7659664264256347\n",
      "Epoch: 47\tError: 1.685726338145234\n",
      "Epoch: 48\tError: 1.6080819103201764\n",
      "Epoch: 49\tError: 1.5329695139009245\n",
      "Epoch: 50\tError: 1.4603273741373657\n",
      "Epoch: 51\tError: 1.3900955043655125\n",
      "Epoch: 52\tError: 1.322215642571727\n",
      "Epoch: 53\tError: 1.2566311906012504\n",
      "Epoch: 54\tError: 1.19328715588494\n",
      "Epoch: 55\tError: 1.1321300955650675\n",
      "Epoch: 56\tError: 1.0731080629071539\n",
      "Epoch: 57\tError: 1.0161705558910297\n",
      "Epoch: 58\tError: 0.9612684678797331\n",
      "Epoch: 59\tError: 0.9083540402703253\n",
      "Epoch: 60\tError: 0.8573808170355818\n",
      "Epoch: 61\tError: 0.8083036010702828\n",
      "Epoch: 62\tError: 0.7610784122602203\n",
      "Epoch: 63\tError: 0.7156624471962502\n",
      "Epoch: 64\tError: 0.6720140404596492\n",
      "Epoch: 65\tError: 0.6300926274087403\n",
      "Epoch: 66\tError: 0.5898587084002829\n",
      "Epoch: 67\tError: 0.5512738143824447\n",
      "Epoch: 68\tError: 0.5143004737992624\n",
      "Epoch: 69\tError: 0.4789021807495138\n",
      "Epoch: 70\tError: 0.44504336434568326\n",
      "Epoch: 71\tError: 0.4126893592213581\n",
      "Epoch: 72\tError: 0.38180637713789767\n",
      "Epoch: 73\tError: 0.35236147964356934\n",
      "Epoch: 74\tError: 0.32432255174062435\n",
      "Epoch: 75\tError: 0.2976582765178498\n",
      "Epoch: 76\tError: 0.27233811070819164\n",
      "Epoch: 77\tError: 0.24833226113293\n",
      "Epoch: 78\tError: 0.22561166199566382\n",
      "Epoch: 79\tError: 0.20414795299114033\n",
      "Epoch: 80\tError: 0.1839134581954901\n",
      "Epoch: 81\tError: 0.16488116570606473\n",
      "Epoch: 82\tError: 0.14702470800045342\n",
      "Epoch: 83\tError: 0.13031834298569145\n",
      "Epoch: 84\tError: 0.1147369357099562\n",
      "Epoch: 85\tError: 0.10025594071031896\n",
      "Epoch: 86\tError: 0.08685138497128132\n",
      "Epoch: 87\tError: 0.07449985146997527\n",
      "Epoch: 88\tError: 0.0631784632849564\n",
      "Epoch: 89\tError: 0.05286486824656057\n",
      "Epoch: 90\tError: 0.04353722410773227\n",
      "Epoch: 91\tError: 0.035174184215180024\n",
      "Epoch: 92\tError: 0.027754883661582807\n",
      "Epoch: 93\tError: 0.021258925900391586\n",
      "Epoch: 94\tError: 0.01566636980558502\n",
      "Epoch: 95\tError: 0.010957717159476915\n",
      "Epoch: 96\tError: 0.007113900552403107\n",
      "Epoch: 97\tError: 0.004116271678791944\n",
      "Epoch: 98\tError: 0.001946590014789537\n",
      "Epoch: 99\tError: 0.0005870118632149986\n"
     ]
    }
   ],
   "source": [
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.0238481 ,  0.1174473 ,  0.02000651,  0.00293277,  0.92239103,\n",
       "          0.00732056,  0.10850985, -0.3501362 ]]),\n",
       " array([[1.10431501, 1.19502222, 0.61940572, 1.04338856, 1.14509088,\n",
       "         1.1403652 , 0.61092989, 0.78603184, 0.14658372]]),\n",
       " array([[3.52320829, 5.01334637, 3.54803971, 4.53163987]]),\n",
       " array([[4.55022833]])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a_l_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " array([[0.5881308 , 0.89771373, 0.89153073, 0.81583748, 0.03588959,\n",
       "         0.69175758, 0.37868094, 0.51851095, 0.65795147],\n",
       "        [0.19385022, 0.2723164 , 0.71860593, 0.78300361, 0.85032764,\n",
       "         0.77524489, 0.03666431, 0.11669374, 0.7512807 ],\n",
       "        [0.23921822, 0.25480601, 0.85762553, 0.94977903, 0.56168686,\n",
       "         0.17878052, 0.77025193, 0.49238104, 0.63125307],\n",
       "        [0.83949792, 0.4610394 , 0.49794007, 0.67941112, 0.65078591,\n",
       "         0.26879524, 0.06732467, 0.77144514, 0.48098413],\n",
       "        [0.32920641, 0.51064106, 0.26362883, 0.31051155, 0.62685344,\n",
       "         0.55744981, 0.31857956, 0.39484322, 0.25797459],\n",
       "        [0.58224112, 0.16162871, 0.59813382, 0.82582358, 0.15639172,\n",
       "         0.73430052, 0.40864343, 0.7786879 , 0.80397057],\n",
       "        [0.78607144, 0.59228702, 0.6644892 , 0.64656729, 0.42563648,\n",
       "         0.51356833, 0.50125784, 0.03708381, 0.7081161 ],\n",
       "        [0.6204306 , 0.77780853, 0.45940947, 0.37980555, 0.2918922 ,\n",
       "         0.55722886, 0.0841636 , 0.63128167, 0.94457049]]),\n",
       " array([[0.69550064, 0.29927188, 0.16563555, 0.33225194],\n",
       "        [0.16543775, 0.91463798, 0.28402501, 0.10760726],\n",
       "        [0.11700177, 0.7555236 , 0.83889446, 0.03553703],\n",
       "        [0.12144377, 0.89100806, 0.51509223, 0.92441901],\n",
       "        [0.58124263, 0.77325639, 0.42267689, 0.82546475],\n",
       "        [0.38540456, 0.35434596, 0.42807852, 0.39506518],\n",
       "        [0.84462971, 0.37382381, 0.37158098, 0.41594296],\n",
       "        [0.42454448, 0.66280484, 0.18977593, 0.72247865],\n",
       "        [0.25366097, 0.96267414, 0.13266125, 0.8067967 ]]),\n",
       " array([[0.00959277],\n",
       "        [0.23757339],\n",
       "        [0.46192478],\n",
       "        [0.76525884]])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " array([[0.88794282, 0.87515968, 0.33956575, 0.69250839, 0.51831089,\n",
       "         0.65416821, 0.26329065, 0.60062043, 0.03991556]]),\n",
       " array([[0.35539385, 0.08285786, 0.72672197, 0.98908231]]),\n",
       " array([[0.9617482]])]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.b_l_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " array([[0.89123753, 0.87858258, 0.34475163, 0.70310053, 0.5273376 ,\n",
       "         0.65931696, 0.26829063, 0.60776708, 0.04815737]]),\n",
       " array([[0.35499545, 0.0846383 , 0.73116878, 0.99660085]]),\n",
       " array([[0.97250331]])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['None'],\n",
       " array([[ 1.88820966e-04,  1.92778871e-04,  2.11484079e-04,\n",
       "          8.45697560e-04,  7.29438774e-04,  4.07467014e-04,\n",
       "          1.85615267e-04,  3.98153768e-04,  8.76477254e-05],\n",
       "        [ 9.29906743e-04,  9.49398657e-04,  1.04151819e-03,\n",
       "          4.16489693e-03,  3.59234490e-03,  2.00669624e-03,\n",
       "          9.14119295e-04,  1.96083031e-03,  4.31648097e-04],\n",
       "        [ 1.58404556e-04,  1.61724897e-04,  1.77416958e-04,\n",
       "          7.09467538e-04,  6.11936413e-04,  3.41829790e-04,\n",
       "          1.55715250e-04,  3.34016777e-04,  7.35289055e-05],\n",
       "        [ 2.32206678e-05,  2.37073997e-05,  2.60077132e-05,\n",
       "          1.04001491e-04,  8.97043151e-05,  5.01091396e-05,\n",
       "          2.28264400e-05,  4.89638230e-05,  1.07786691e-05],\n",
       "        [ 7.30317003e-03,  7.45625286e-03,  8.17972823e-03,\n",
       "          3.27096783e-02,  2.82130502e-02,  1.57599071e-02,\n",
       "          7.17918081e-03,  1.53996917e-02,  3.39001676e-03],\n",
       "        [ 5.79616669e-05,  5.91766100e-05,  6.49184780e-05,\n",
       "          2.59600622e-04,  2.23913097e-04,  1.25078628e-04,\n",
       "          5.69776255e-05,  1.22219775e-04,  2.69048950e-05],\n",
       "        [ 8.59143108e-04,  8.77151734e-04,  9.62261196e-04,\n",
       "          3.84795843e-03,  3.31897621e-03,  1.85399155e-03,\n",
       "          8.44557047e-04,  1.81161590e-03,  3.98800730e-04],\n",
       "        [-2.77225613e-03, -2.83036580e-03, -3.10499435e-03,\n",
       "         -1.24164720e-02, -1.07095687e-02, -5.98240199e-03,\n",
       "         -2.72519028e-03, -5.84566557e-03, -1.28683773e-03]]),\n",
       " array([[-0.02244277,  0.01011869,  0.06261063,  0.14173798],\n",
       "        [-0.02428619,  0.01094983,  0.0677534 ,  0.15338018],\n",
       "        [-0.01258805,  0.00567553,  0.03511804,  0.07950025],\n",
       "        [-0.02120457,  0.00956043,  0.05915632,  0.13391812],\n",
       "        [-0.02327145,  0.01049231,  0.06492247,  0.14697153],\n",
       "        [-0.02317541,  0.01044901,  0.06465454,  0.146365  ],\n",
       "        [-0.0124158 ,  0.00559787,  0.03463749,  0.07841238],\n",
       "        [-0.01597436,  0.0072023 ,  0.04456514,  0.10088658],\n",
       "        [-0.00297899,  0.00134313,  0.00831076,  0.01881391]]),\n",
       " array([[23.01080433],\n",
       "        [36.57174132],\n",
       "        [23.2367816 ],\n",
       "        [32.18799223]])]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dl_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "dldw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 2, 0, 0],\n",
       "       [0, 0, 3, 0],\n",
       "       [0, 0, 0, 4]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "np.diag(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE -> Generalise for any loss\n",
    "Softmax backProp\n",
    "Stopping condition stochastic gd\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_331",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
