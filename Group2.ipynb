{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    #List of List for each layer along with their information\n",
    "    #Shape: [ [layerType,activationF(),neuronCount],[layerType,activationF(),neuronCount]]\n",
    "    layer_info = None\n",
    "    #List of weight matrices of each layer\n",
    "    #Shape: dxk (where d is no of input)(k is no of output)\n",
    "    w_l_ij= None\n",
    "    #List of bias vector of each layer\n",
    "    #Shape: 1xk (where k is no of output/neuron)\n",
    "    b_l_j = None\n",
    "    #List of output of each neuron\n",
    "    #Shape: 1xd (where d is dimension)\n",
    "    a_l_ij = None\n",
    "    #List of d(a_l+1)/da_l (where a_l+1 is output of layer)(a_l is input of layer)\n",
    "    #Shape: 1xd (same shape as input of layer)\n",
    "    delta_l_ij = None\n",
    "    #List of matrices of dl/dw for each layer\n",
    "    dl_db = None\n",
    "    dl_dw = None\n",
    "    #Dictionary for differentiation function for a input function. diff[f()] => df()\n",
    "    diff = {}\n",
    "    input_featureCount = None\n",
    "    input_count = None\n",
    "    def __init__(self,xShape,rseed = 10,listLayer= None):\n",
    "        np.random.seed(rseed)\n",
    "        inputCount,inputDim = xShape\n",
    "        self.input_featureCount = inputDim\n",
    "        self.input_count = inputCount\n",
    "        self.layer_info = [['None','None',self.input_featureCount]]\n",
    "        self.w_l_ij = ['None']\n",
    "        self.b_l_j = ['None']\n",
    "        self.optimizer = None\n",
    "        self.diff[ANN.activation_linear] = ANN.diff_activation_linear\n",
    "        self.diff[ANN.activation_sigmoid] = ANN.diff_activation_sigmoid\n",
    "        self.diff[ANN.activation_tanh] = ANN.diff_activation_tanh\n",
    "\n",
    "    def compile(self,optimizer,lossFunction):\n",
    "        self.optimizer = optimizer\n",
    "        self.layer_info.append(['Loss',lossFunction])\n",
    "\n",
    "    def __str__(self):\n",
    "        return\n",
    "\n",
    "    def addLayers_Dense(self,neuronCount,activationFunction):\n",
    "            self.layer_info.append(['Dense',activationFunction,neuronCount])\n",
    "            w = np.random.rand(self.layer_info[-2][2],neuronCount)\n",
    "            b = np.random.rand(1,neuronCount)\n",
    "            self.w_l_ij.append(w)\n",
    "            self.b_l_j.append(b)\n",
    "\n",
    "    def addLayers_softmax(self):\n",
    "        self.layer_info.append(['Softmax',self.layer_info[-1][2]])\n",
    "\n",
    "    def forward_layer_dense(self,layerNo):\n",
    "        if layerNo < 1:\n",
    "            print(\"ERRRROR\")\n",
    "        \n",
    "        #Output = activationFunction(a_l-1 @ w_l + b_l)\n",
    "        output = self.layer_info[layerNo][1](self.a_l_ij[layerNo-1] @ self.w_l_ij[layerNo] + self.b_l_j[layerNo]) \n",
    "        self.a_l_ij.append(output)\n",
    "    \n",
    "    def backward_layer_dense(self,layerNo):\n",
    "        #at each layer we need to calculate da_l/da_l-1, and da_l/dw_l\n",
    "\n",
    "        #Shape: dxk\n",
    "        da_1da = self.w_l_ij[layerNo]\n",
    "        #Shape: dx1\n",
    "        da_l_1dw_l = self.a_l_ij[layerNo-1].T\n",
    "\n",
    "        #multiply both by dL/da_l to get dL/da_l-1 and dL/dw_l\n",
    "        self.delta_l_ij[layerNo] = self.delta_l_ij[layerNo+1] @ da_1da.T \n",
    "        self.dl_dw.append(da_l_1dw_l @ self.delta_l_ij[layerNo+1])\n",
    "        self.dl_db.append(self.delta_l_ij[layerNo+1])\n",
    "    \n",
    "    def forward_layer_softmax(self,layerNo):\n",
    "        if layerNo < 1: \n",
    "            print(\"ERRORRR\")\n",
    "        denom = np.sum( np.exp(self.a_l_ij[layerNo-1]) )\n",
    "        output = np.exp(self.a_l_ij[layerNo]) / denom\n",
    "        self.a_l_ij.append(output)\n",
    "\n",
    "    def backward_layer_softmax(self,layerNo):\n",
    "        pass\n",
    "\n",
    "    #Create cross entropy loss \n",
    "    def forward_layer_loss(self,layerNo,y):\n",
    "        y_hat  =  self.a_l_ij[-1]\n",
    "        lossFunction = self.layer_info[layerNo][1]\n",
    "        return lossFunction(y_hat,y)\n",
    "\n",
    "    def forwardProp(self,x,y):\n",
    "        self.a_l_ij = []\n",
    "        self.a_l_ij.append(x)\n",
    "        for layerNo in range(len(self.layer_info)):\n",
    "            if self.layer_info[layerNo][0] == 'Dense':\n",
    "                self.forward_layer_dense(layerNo)\n",
    "            if self.layer_info[layerNo][0] == 'Softmax':\n",
    "                self.forward_layer_softmax(layerNo)\n",
    "            if self.layer_info[layerNo][0] == 'Loss':\n",
    "                cost = self.forward_layer_loss(layerNo,y)\n",
    "                return cost\n",
    "            \n",
    "    #Handle softmax backpropogation\n",
    "    def backwardProp(self,y):\n",
    "        self.dl_dw = []\n",
    "        self.dl_db = []\n",
    "        lastIndex = len(self.layer_info) - 2\n",
    "        self.delta_l_ij = [i for i in range(0,lastIndex+1)]\n",
    "        \n",
    "        #MSE LOSS And Dense\n",
    "        self.delta_l_ij[lastIndex] = 2 * ( (self.a_l_ij[lastIndex] - y) @ self.w_l_ij[lastIndex].T) \n",
    "        self.dl_dw.append(self.a_l_ij[lastIndex-1].T @ (self.a_l_ij[lastIndex] - y))\n",
    "        self.dl_db.append(2*(self.a_l_ij[lastIndex] - y))\n",
    "        #BackPropogation\n",
    "        for layerNo in range(lastIndex-1,0,-1):\n",
    "            if self.layer_info[layerNo][0] == 'Dense':\n",
    "                self.backward_layer_dense(layerNo)\n",
    "            if self.layer_info[layerNo][0] == 'Softmax':\n",
    "                self.backward_layer_softmax(layerNo)\n",
    "                \n",
    "        self.dl_dw.append(['None'])\n",
    "        self.dl_db.append(['None'])\n",
    "        self.dl_dw.reverse()\n",
    "        self.dl_db.reverse()\n",
    "\n",
    "\n",
    "            \n",
    "    def fit(self,x,y):\n",
    "        #Hyperparameters if required: \n",
    "        epoch = 10\n",
    "        eta = 1e-6\n",
    "        self.optimizer(self,x,y,eta,epoch)\n",
    "\n",
    "    @staticmethod    \n",
    "    def optimizer_gradientDescent(Obj,x,y,eta,epoch):\n",
    "        for i in  range(epoch):\n",
    "            errorSum = 0\n",
    "            print(\"Epoch:\",i,end=\"\\t[\")\n",
    "            for j in range(y.shape[0]):\n",
    "                #x and y both are 2d matrix\n",
    "                x_1xd = x[j:j+1]\n",
    "                y_1xk = y[j:j+1]\n",
    "                errorSum += Obj.forwardProp(x_1xd,y_1xk)\n",
    "                Obj.backwardProp(y_1xk)\n",
    "                for index in range(1,len(Obj.w_l_ij)):\n",
    "                    Obj.w_l_ij[index] = Obj.w_l_ij[index] - (eta * Obj.dl_dw[index])\n",
    "                    Obj.b_l_j[index] = Obj.b_l_j[index] - (eta * Obj.dl_db[index] )\n",
    "            print(\"]\", end='\\t')\n",
    "            print(\"Error:\",errorSum)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_MSE(P,y):\n",
    "        return np.sum((P - y)**2)   \n",
    "      \n",
    "    @staticmethod\n",
    "    def activation_linear(z):\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def diff_activation_linear(x):\n",
    "        return np.ones(x.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def activation_sigmoid(z):    \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod    \n",
    "    def diff_activation_sigmoid(a):\n",
    "        return a * (1- a)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_tanh(z):\n",
    "        sinh = np.exp(z) - np.exp(-z)\n",
    "        cosh = np.exp(z) + np.exp(-z)\n",
    "        return sinh / cosh\n",
    "    \n",
    "    @staticmethod\n",
    "    def diff_activation_tanh(z):\n",
    "        return 1 - (z * z)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_relu(z):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.datasets as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "          37.88      , -122.23      ],\n",
       "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "          37.86      , -122.22      ],\n",
       "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "          37.85      , -122.24      ],\n",
       "       ...,\n",
       "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "          39.43      , -121.22      ],\n",
       "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "          39.43      , -121.32      ],\n",
       "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "          39.37      , -121.24      ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictt = sk.fetch_california_housing()\n",
    "x = dictt.data\n",
    "y = np.array(dictt.target)\n",
    "y = y.T\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0238481 ,  0.1174473 ,  0.02000651, ...,  0.00732056,\n",
       "         0.10850985, -0.3501362 ],\n",
       "       [ 0.00345241,  0.00873354,  0.00259434, ...,  0.00087745,\n",
       "         0.01574533, -0.05082923],\n",
       "       [ 0.01409202,  0.10097076,  0.01609345, ...,  0.00544128,\n",
       "         0.07349506, -0.23735895],\n",
       "       ...,\n",
       "       [ 0.00167455,  0.01674553,  0.00512762, ...,  0.00229082,\n",
       "         0.03883978, -0.11940547],\n",
       "       [ 0.00248251,  0.02393168,  0.00708579, ...,  0.00282289,\n",
       "         0.05242368, -0.16129955],\n",
       "       [ 0.00171478,  0.0114864 ,  0.00377236, ...,  0.00187873,\n",
       "         0.02826371, -0.08703817]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "transformer = Normalizer().fit(x)\n",
    "x = transformer.transform(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(x.shape,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.addLayers_Dense(4,ANN.activation_linear)\n",
    "model.addLayers_Dense(13,ANN.activation_linear)\n",
    "model.addLayers_Dense(1,ANN.activation_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(ANN.optimizer_gradientDescent,ANN.loss_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['None', 'None', 8],\n",
       " ['Dense', <function __main__.ANN.activation_linear(z)>, 4],\n",
       " ['Dense', <function __main__.ANN.activation_linear(z)>, 13],\n",
       " ['Dense', <function __main__.ANN.activation_linear(z)>, 1],\n",
       " ['Loss', <function __main__.ANN.loss_MSE(P, y)>]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "w_old = copy.deepcopy(model.w_l_ij)\n",
    "b_old = copy.deepcopy(model.b_l_j)\n",
    "dldw = copy.deepcopy(model.dl_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t["
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "]\tError: 366688.9119047099\n",
      "Epoch: 1\t[]\tError: 31480.999319046063\n",
      "Epoch: 2\t[]\tError: 28374.784701982655\n",
      "Epoch: 3\t[]\tError: 28224.191505985633\n",
      "Epoch: 4\t[]\tError: 28166.07273035525\n",
      "Epoch: 5\t[]\tError: 28117.240705890184\n",
      "Epoch: 6\t[]\tError: 28071.190699136645\n",
      "Epoch: 7\t[]\tError: 28027.126672421728\n",
      "Epoch: 8\t[]\tError: 27984.879193785553\n",
      "Epoch: 9\t[]\tError: 27944.355682917587\n"
     ]
    }
   ],
   "source": [
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " array([[ 0.5892334 ,  0.89892337,  0.89271579,  0.81687445],\n",
       "        [ 0.03109845,  0.68585777,  0.37241143,  0.51348419],\n",
       "        [ 0.65724609,  0.19296765,  0.27136908,  0.7178546 ],\n",
       "        [ 0.7827302 ,  0.8499998 ,  0.77490251,  0.03638461],\n",
       "        [-0.23760754,  0.33590743, -0.18794221, -0.09996116],\n",
       "        [ 0.85690945,  0.94892526,  0.5607987 ,  0.17805196],\n",
       "        [ 0.76416856,  0.48496005,  0.62341497,  0.83317238],\n",
       "        [ 0.47944692,  0.52052026,  0.70334518,  0.67002714]]),\n",
       " array([[0.32808148, 0.50952174, 0.26204796, 0.30903368, 0.62485426,\n",
       "         0.55644286, 0.31705974, 0.39359329, 0.25639396, 0.58029745,\n",
       "         0.16053744, 0.59605947, 0.82478461],\n",
       "        [0.13625614, 0.71621244, 0.3872589 , 0.75898133, 0.75802996,\n",
       "         0.76626419, 0.57423962, 0.64270821, 0.62530774, 0.39079684,\n",
       "         0.50545953, 0.46324134, 0.02475277],\n",
       "        [0.68021579, 0.59541138, 0.74832178, 0.43224367, 0.31592862,\n",
       "         0.26440903, 0.53241453, 0.05399259, 0.60196876, 0.8962901 ,\n",
       "         0.88024429, 0.8258842 , 0.3277973 ],\n",
       "        [0.68475085, 0.5108568 , 0.63982935, 0.25033164, 0.56589322,\n",
       "         0.0301063 , 0.67905563, 0.27942008, 0.14626027, 0.30049827,\n",
       "         0.15805219, 0.87999038, 0.27278889]]),\n",
       " array([[0.33013105],\n",
       "        [0.28961977],\n",
       "        [0.31804403],\n",
       "        [0.29087453],\n",
       "        [0.79502362],\n",
       "        [0.33267349],\n",
       "        [0.25387892],\n",
       "        [0.35222368],\n",
       "        [0.3141291 ],\n",
       "        [0.56821484],\n",
       "        [0.08192956],\n",
       "        [0.62519849],\n",
       "        [0.17378417]])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w_l_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " array([[0.5881308 , 0.89771373, 0.89153073, 0.81583748],\n",
       "        [0.03588959, 0.69175758, 0.37868094, 0.51851095],\n",
       "        [0.65795147, 0.19385022, 0.2723164 , 0.71860593],\n",
       "        [0.78300361, 0.85032764, 0.77524489, 0.03666431],\n",
       "        [0.11669374, 0.7512807 , 0.23921822, 0.25480601],\n",
       "        [0.85762553, 0.94977903, 0.56168686, 0.17878052],\n",
       "        [0.77025193, 0.49238104, 0.63125307, 0.83949792],\n",
       "        [0.4610394 , 0.49794007, 0.67941112, 0.65078591]]),\n",
       " array([[0.32920641, 0.51064106, 0.26362883, 0.31051155, 0.62685344,\n",
       "         0.55744981, 0.31857956, 0.39484322, 0.25797459, 0.58224112,\n",
       "         0.16162871, 0.59813382, 0.82582358],\n",
       "        [0.15639172, 0.73430052, 0.40864343, 0.7786879 , 0.80397057,\n",
       "         0.78607144, 0.59228702, 0.6644892 , 0.64656729, 0.42563648,\n",
       "         0.51356833, 0.50125784, 0.03708381],\n",
       "        [0.7081161 , 0.6204306 , 0.77780853, 0.45940947, 0.37980555,\n",
       "         0.2918922 , 0.55722886, 0.0841636 , 0.63128167, 0.94457049,\n",
       "         0.89123753, 0.87858258, 0.34475163],\n",
       "        [0.70310053, 0.5273376 , 0.65931696, 0.26829063, 0.60776708,\n",
       "         0.04815737, 0.69550064, 0.29927188, 0.16563555, 0.33225194,\n",
       "         0.16543775, 0.91463798, 0.28402501]]),\n",
       " array([[0.38540456],\n",
       "        [0.35434596],\n",
       "        [0.42807852],\n",
       "        [0.39506518],\n",
       "        [0.84462971],\n",
       "        [0.37382381],\n",
       "        [0.37158098],\n",
       "        [0.41594296],\n",
       "        [0.42454448],\n",
       "        [0.66280484],\n",
       "        [0.18977593],\n",
       "        [0.72247865],\n",
       "        [0.25366097]])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " array([[-0.07904818, -0.34092588,  0.35129059,  0.13232068]]),\n",
       " array([[ 0.05755212,  0.07234864,  0.70340903,  0.790923  , -0.08024858,\n",
       "          0.07193192,  0.84753706,  0.46102015,  0.8726238 ,  0.49460246,\n",
       "          0.75484849,  0.32801989,  0.79576466]]),\n",
       " array([[0.81965494]])]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.b_l_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " array([[0.26879524, 0.06732467, 0.77144514, 0.48098413]]),\n",
       " array([[0.10760726, 0.11700177, 0.7555236 , 0.83889446, 0.03553703,\n",
       "         0.12144377, 0.89100806, 0.51509223, 0.92441901, 0.58124263,\n",
       "         0.77325639, 0.42267689, 0.82546475]]),\n",
       " array([[0.96267414]])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['None'],\n",
       " array([[ 9.78929005e-03,  1.11279425e-02,  1.12054356e-02,\n",
       "          9.52499771e-03],\n",
       "        [ 6.55734074e-02,  7.45403498e-02,  7.50594361e-02,\n",
       "          6.38030492e-02],\n",
       "        [ 2.15356061e-02,  2.44805276e-02,  2.46510058e-02,\n",
       "          2.09541854e-02],\n",
       "        [ 4.76335129e-03,  5.41472352e-03,  5.45243073e-03,\n",
       "          4.63474980e-03],\n",
       "        [ 5.68439475e+00,  6.46171657e+00,  6.50671486e+00,\n",
       "          5.53092683e+00],\n",
       "        [ 1.07252731e-02,  1.21919181e-02,  1.22768205e-02,\n",
       "          1.04357110e-02],\n",
       "        [ 1.61351566e-01,  1.83415848e-01,  1.84693125e-01,\n",
       "          1.56995378e-01],\n",
       "        [-4.96882494e-01, -5.64829500e-01, -5.68762877e-01,\n",
       "         -4.83467606e-01]]),\n",
       " array([[-0.27374504, -0.24015297, -0.26372317, -0.24119421, -0.65923413,\n",
       "         -0.2758531 , -0.2105175 , -0.29206457, -0.26047706, -0.47116475,\n",
       "         -0.06793692, -0.51841547, -0.1441026 ],\n",
       "        [-0.02109217, -0.01850389, -0.02031998, -0.01858411, -0.05079426,\n",
       "         -0.0212546 , -0.01622046, -0.0225037 , -0.02006987, -0.03630344,\n",
       "         -0.00523457, -0.03994413, -0.01110317],\n",
       "        [ 0.10720496,  0.09404952,  0.10328016,  0.09445729,  0.25817149,\n",
       "          0.10803052,  0.08244357,  0.11437931,  0.1020089 ,  0.18451913,\n",
       "          0.02660568,  0.20302362,  0.05643395],\n",
       "        [ 0.00697551,  0.00611953,  0.00672014,  0.00614606,  0.01679846,\n",
       "          0.00702923,  0.00536436,  0.00744233,  0.00663742,  0.01200612,\n",
       "          0.00173115,  0.01321015,  0.00367199]]),\n",
       " array([[ 0.04968774],\n",
       "        [-0.0410615 ],\n",
       "        [ 0.89192184],\n",
       "        [ 0.91519696],\n",
       "        [-0.32693073],\n",
       "        [-0.12146084],\n",
       "        [ 1.0097679 ],\n",
       "        [ 0.40782747],\n",
       "        [ 1.07050475],\n",
       "        [ 0.51768849],\n",
       "        [ 1.01125659],\n",
       "        [ 0.29401993],\n",
       "        [ 0.71418889]])]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dl_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dldw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.delta_l_ij = [i for i in range(0,lastIndex+1)]\n",
    "# self.delta_l_ij[lastIndex] = (2 * (self.a_l_ij[lastIndex] - y).T * self.diff[self.layer_info[lastIndex][1]](self.a_l_ij[lastIndex]))\n",
    "# self.dl_dw.append(self.a_l_ij[lastIndex-1].T @ self.delta_l_ij[lastIndex])\n",
    "# for layerNo in range(lastIndex-1,0,-1):\n",
    "#     self.delta_l_ij[layerNo] = (self.w_l_ij[layerNo+1] @self.delta_l_ij[layerNo+1].T).T * self.diff[self.layer_info[layerNo][1]](self.a_l_ij[layerNo])\n",
    "#     self.dl_dw.append(self.a_l_ij[layerNo-1].T @ self.delta_l_ij[layerNo])\n",
    "# self.dl_dw.append(['None'])\n",
    "# self.dl_dw.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # self.delta_l_ij = [i for i in range(0,lastIndex+1)]\n",
    "        # self.delta_l_ij[lastIndex] = (2 * (self.a_l_ij[lastIndex] - y).T * self.diff[self.layer_info[lastIndex][1]](self.a_l_ij[lastIndex]))\n",
    "        # self.dl_dw.append(self.a_l_ij[lastIndex-1].T @ self.delta_l_ij[lastIndex])\n",
    "        # self.dl_db.append(self.a_l_ij[lastIndex])\n",
    "        # for layerNo in range(lastIndex-1,0,-1):\n",
    "        #     self.delta_l_ij[layerNo] = (self.w_l_ij[layerNo+1] @self.delta_l_ij[layerNo+1].T).T * self.diff[self.layer_info[layerNo][1]](self.a_l_ij[layerNo])\n",
    "        #     self.dl_dw.append(self.a_l_ij[layerNo-1].T @ self.delta_l_ij[layerNo])\n",
    "        #     self.dl_db.append(self.delta_l_ij[layerNo])\n",
    "        # self.dl_dw.append(['None'])\n",
    "        # self.dl_db.append(['None'])\n",
    "        # self.dl_dw.reverse()\n",
    "        # self.dl_dw.reverse()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_331",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
