{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN:\n",
    "    #List of List for each layer along with their information\n",
    "    #Shape: [ [layerType,activationF(),neuronCount],[layerType,activationF(),neuronCount]]\n",
    "    layer_info = None\n",
    "    #List of weight matrices of each layer\n",
    "    #Shape: dxk (where d is no of input)(k is no of output)\n",
    "    w_l_ij= None\n",
    "    #List of bias vector of each layer\n",
    "    #Shape: 1xk (where k is no of output/neuron)\n",
    "    b_l_j = None\n",
    "    #List of output of each neuron\n",
    "    #Shape: 1xd (where d is dimension)\n",
    "    a_l_ij = None\n",
    "    #List of d(a_l+1)/da_l (where a_l+1 is output of layer)(a_l is input of layer)\n",
    "    #Shape: 1xd (same shape as input of layer)\n",
    "    delta_l_ij = None\n",
    "    #List of matrices of dl/dw for each layer\n",
    "    dl_db = None\n",
    "    dl_dw = None\n",
    "    #Dictionary for differentiation function for a input function. diff[f()] => df()\n",
    "    diff = {}\n",
    "    input_featureCount = None\n",
    "    input_count = None\n",
    "    da_da = None\n",
    "    def __init__(self,xShape,rseed = 45,listLayer= None):\n",
    "        np.random.seed(rseed)\n",
    "        inputCount,inputDim = xShape\n",
    "        self.input_featureCount = inputDim\n",
    "        self.input_count = inputCount\n",
    "        self.layer_info = [['None','None',self.input_featureCount]]\n",
    "        self.w_l_ij = ['None']\n",
    "        self.b_l_j = ['None']\n",
    "        self.optimizer = None\n",
    "        self.da_da = []\n",
    "        self.diff[ANN.activation_linear] = ANN.diff_activation_linear\n",
    "        self.diff[ANN.activation_sigmoid] = ANN.diff_activation_sigmoid\n",
    "        self.diff[ANN.activation_tanh] = ANN.diff_activation_tanh\n",
    "\n",
    "    def compile(self,optimizer,lossFunction):\n",
    "        self.optimizer = optimizer\n",
    "        self.layer_info.append(['Loss',lossFunction])\n",
    "\n",
    "    def __str__(self):\n",
    "        return\n",
    "\n",
    "    def addLayers_Dense(self,neuronCount,activationFunction):\n",
    "            self.layer_info.append(['Dense',activationFunction,neuronCount])\n",
    "            w = np.random.normal(0,10,(self.layer_info[-2][2],neuronCount))\n",
    "            b = np.random.normal(0,10,(1,neuronCount))\n",
    "            self.w_l_ij.append(w)\n",
    "            self.b_l_j.append(b)\n",
    "\n",
    "    def addLayers_softmax(self):\n",
    "        self.layer_info.append(['Softmax',self.layer_info[-1][2]])\n",
    "        self.w_l_ij.append('None')\n",
    "        self.b_l_j.append('None')\n",
    "\n",
    "    def forward_layer_dense(self,layerNo):\n",
    "        if layerNo < 1:\n",
    "            print(\"ERRRROR\")\n",
    "        #Output = activationFunction(a_l-1 @ w_l + b_l)\n",
    "        output = self.layer_info[layerNo][1](self.a_l_ij[layerNo-1] @ self.w_l_ij[layerNo] + self.b_l_j[layerNo]) \n",
    "        self.a_l_ij.append(output)\n",
    "    \n",
    "    def backward_layer_dense(self,layerNo):\n",
    "        #at each layer we need to calculate da_l/da_l-1, and da_l/dw_l\n",
    "        #Shape: dxk\n",
    "        da_1da = self.diff[self.layer_info[layerNo][1]](self.a_l_ij[layerNo]).T @ self.w_l_ij[layerNo].T\n",
    "        self.da_da.append(da_1da)\n",
    "        #Shape: dxk\n",
    "        da_l1_dw_l = np.zeros(self.w_l_ij[layerNo].shape)\n",
    "        for i in range(da_l1_dw_l.shape[0]):\n",
    "            for j in range(da_l1_dw_l.shape[1]):\n",
    "                da_l1_dw_l[i][j] = self.diff[self.layer_info[layerNo][1]](self.a_l_ij[layerNo][0][j],'SINGLE') * self.a_l_ij[layerNo-1][0][i]\n",
    "\n",
    "        #multiply both by dL/da_l to get dL/da_l-1 and dL/dw_l\n",
    "        #print(layerNo)\n",
    "        #print(da_1da)\n",
    "        #print()\n",
    "        self.delta_l_ij[layerNo] = self.delta_l_ij[layerNo+1] @ da_1da\n",
    "        #print(\"AAA:\",da_l1_dw_l,\"\\n\", self.delta_l_ij[layerNo+1])\n",
    "        temp = np.zeros(self.w_l_ij[layerNo].shape)\n",
    "        for i in range(temp.shape[0]):\n",
    "            for j in range(temp.shape[1]):\n",
    "                temp[i][j] = da_l1_dw_l[i][j] * self.delta_l_ij[layerNo+1][0][j]\n",
    "\n",
    "        self.dl_dw.append(temp)\n",
    "        self.dl_db.append(self.delta_l_ij[layerNo+1])\n",
    "    \n",
    "    def forward_layer_softmax(self,layerNo):\n",
    "        if layerNo < 1: \n",
    "            print(\"ERRORRR\")\n",
    "        denom = np.sum( np.exp(self.a_l_ij[layerNo-1]) )\n",
    "        output = np.exp(self.a_l_ij[layerNo-1]) / denom\n",
    "        self.a_l_ij.append(output)\n",
    "\n",
    "    def backward_layer_softmax(self,layerNo):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def forward_layer_loss(self,layerNo,y):\n",
    "        y_hat  =  self.a_l_ij[-1]\n",
    "        lossFunction = self.layer_info[layerNo][1]\n",
    "        return lossFunction(y_hat,y)\n",
    "\n",
    "    def forwardProp(self,x,y):\n",
    "        self.a_l_ij = []\n",
    "        self.a_l_ij.append(x)\n",
    "        for layerNo in range(len(self.layer_info)):\n",
    "            if self.layer_info[layerNo][0] == 'Dense':\n",
    "                self.forward_layer_dense(layerNo)\n",
    "            if self.layer_info[layerNo][0] == 'Softmax':\n",
    "                self.forward_layer_softmax(layerNo)\n",
    "            if self.layer_info[layerNo][0] == 'Loss':\n",
    "                cost = self.forward_layer_loss(layerNo,y)\n",
    "                return cost\n",
    "            \n",
    "    #Handle softmax backpropogation\n",
    "    def backwardProp(self,y):\n",
    "        self.dl_dw = []\n",
    "        self.dl_db = []\n",
    "        #-1 for getting index, -1 for loss layer\n",
    "        lastIndex = len(self.layer_info) - 2\n",
    "        self.delta_l_ij = [i for i in range(0,lastIndex+1)]\n",
    "        \n",
    "        #MSE LOSS And Dense\n",
    "        if(self.layer_info[-1][1] == ANN.loss_MSE):\n",
    "            self.delta_l_ij[lastIndex] = 2 * ( (self.a_l_ij[lastIndex] - y) @ self.w_l_ij[lastIndex].T) \n",
    "            self.dl_dw.append(2*(self.a_l_ij[lastIndex-1].T @ self.a_l_ij[lastIndex] - y))\n",
    "            self.dl_db.append(2*(self.a_l_ij[lastIndex] - y))\n",
    "        elif(self.layer_info[-1][1] == ANN.loss_crossEntropy):\n",
    "            self.dl_dw.append(['None'])\n",
    "            self.delta_l_ij[lastIndex] = self.a_l_ij[lastIndex] - y \n",
    "             \n",
    "        #BackPropogation\n",
    "        for layerNo in range(lastIndex-1,0,-1):\n",
    "            if self.layer_info[layerNo][0] == 'Dense':\n",
    "                self.backward_layer_dense(layerNo)\n",
    "                \n",
    "        self.dl_dw.append(['None'])\n",
    "        self.dl_db.append(['None'])\n",
    "        self.dl_dw.reverse()\n",
    "        self.dl_db.reverse()\n",
    "\n",
    "    def fit(self,x,y,eta):\n",
    "        #Hyperparameters if required: \n",
    "        epoch = 1000\n",
    "        tolerance = 1e-5\n",
    "        self.optimizer(self,x,y,eta,epoch,tolerance)\n",
    "\n",
    "    @staticmethod    \n",
    "    def optimizer_gradientDescent(Obj,x,y,eta,epoch,tolerance):\n",
    "        for i in  range(epoch):\n",
    "            errorSum = 0\n",
    "            # print(\"Epoch:\",i,end=\"\\t\")\n",
    "            for j in range(1):\n",
    "                #x and y both are 2d matrix\n",
    "                x_1xd = x[j:j+1]\n",
    "                y_1xk = y[j:j+1]\n",
    "                errorSum += Obj.forwardProp(x_1xd,y_1xk)\n",
    "                Obj.backwardProp(y_1xk)\n",
    "                for index in range(1,len(Obj.w_l_ij)):\n",
    "                    if( 'None' in Obj.dl_dw[index]):\n",
    "                        continue\n",
    "                    Obj.w_l_ij[index] = Obj.w_l_ij[index] - (eta * Obj.dl_dw[index])\n",
    "                    Obj.b_l_j[index] = Obj.b_l_j[index] - (eta * Obj.dl_db[index] )\n",
    "            # print(\"Error:\",errorSum)\n",
    "            if(errorSum < tolerance):\n",
    "                return\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_MSE(P,y):\n",
    "        return np.sum((P - y)**2)   \n",
    "\n",
    "    @staticmethod\n",
    "    def loss_crossEntropy(P,y):\n",
    "        loss = np.log(P ** y)\n",
    "        return -np.sum(loss)\n",
    "      \n",
    "    @staticmethod\n",
    "    def activation_linear(z):\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def diff_activation_linear(x,flg='None'):\n",
    "        if(flg != 'SINGLE'):\n",
    "            return np.identity(x.shape[1])\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def activation_sigmoid(z):    \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod    \n",
    "    def diff_activation_sigmoid(a,flg='None'):\n",
    "        if(flg !='SINGLE'):\n",
    "            x = a*(1-a)\n",
    "            return np.diag(x[0])\n",
    "        else:\n",
    "            return a * (1- a)\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_tanh(z):\n",
    "        sinh = np.exp(z) - np.exp(-z)\n",
    "        cosh = np.exp(z) + np.exp(-z)\n",
    "        return sinh / cosh\n",
    "    \n",
    "    @staticmethod\n",
    "    def diff_activation_tanh(z,flg='None'):\n",
    "        if(flg != 'SINGLE'):\n",
    "            x = 1 - (z*z)\n",
    "            return np.diag(x[0])\n",
    "        else:\n",
    "            return 1 - (z * z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.datasets as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
       "          37.88      , -122.23      ],\n",
       "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
       "          37.86      , -122.22      ],\n",
       "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
       "          37.85      , -122.24      ],\n",
       "       ...,\n",
       "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
       "          39.43      , -121.22      ],\n",
       "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
       "          39.43      , -121.32      ],\n",
       "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
       "          39.37      , -121.24      ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictt = sk.fetch_california_housing()\n",
    "x = dictt.data\n",
    "y = np.array(dictt.target)\n",
    "y = y.T\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "transformer = Normalizer().fit(x)\n",
    "x = transformer.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(x.shape,42)\n",
    "model.addLayers_Dense(1,ANN.activation_linear)\n",
    "model.compile(ANN.optimizer_gradientDescent,ANN.loss_MSE)\n",
    "model.fit(x,y,1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.0238481 ,  0.1174473 ,  0.02000651,  0.00293277,  0.92239103,\n",
       "          0.00732056,  0.10850985, -0.3501362 ]]),\n",
       " array([[4.19180001]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a_l_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(x.shape,42)\n",
    "model.addLayers_Dense(13,ANN.activation_sigmoid)\n",
    "model.addLayers_Dense(1,ANN.activation_linear)\n",
    "model.compile(ANN.optimizer_gradientDescent,ANN.loss_MSE)\n",
    "model.fit(x,y,1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.0238481 ,  0.1174473 ,  0.02000651,  0.00293277,  0.92239103,\n",
       "          0.00732056,  0.10850985, -0.3501362 ]]),\n",
       " array([[4.09401803e-08, 9.99991341e-01, 1.00000000e+00, 9.99998831e-01,\n",
       "         5.73691430e-01, 6.47717916e-02, 6.05984694e-11, 9.99990562e-01,\n",
       "         6.39842892e-02, 1.00000000e+00, 5.03230576e-07, 1.30622857e-03,\n",
       "         9.99639474e-01]]),\n",
       " array([[4.52697958]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a_l_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(x.shape,42)\n",
    "model.addLayers_Dense(13,ANN.activation_sigmoid)\n",
    "model.addLayers_Dense(13,ANN.activation_sigmoid)\n",
    "model.addLayers_Dense(1,ANN.activation_linear)\n",
    "model.compile(ANN.optimizer_gradientDescent,ANN.loss_MSE)\n",
    "model.fit(x,y,1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.0238481 ,  0.1174473 ,  0.02000651,  0.00293277,  0.92239103,\n",
       "          0.00732056,  0.10850985, -0.3501362 ]]),\n",
       " array([[9.99933009e-01, 9.48199636e-14, 1.00000000e+00, 1.49949124e-51,\n",
       "         3.32109711e-13, 1.00000000e+00, 3.24557519e-08, 9.99994845e-01,\n",
       "         9.99999993e-01, 1.00000000e+00, 2.37250354e-32, 1.00000000e+00,\n",
       "         1.39303035e-63]]),\n",
       " array([[7.36336000e-43, 7.10753885e-05, 8.14451596e-24, 6.38091956e-37,\n",
       "         5.79508715e-38, 9.19352551e-44, 1.17542481e-28, 5.08279735e-49,\n",
       "         3.69030153e-31, 1.05842797e-31, 2.28821558e-31, 5.48591637e-57,\n",
       "         5.44663293e-33]]),\n",
       " array([[4.52911511]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.a_l_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictt = sk.load_digits()\n",
    "x = dictt.data\n",
    "y = dictt.target\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "transformer = Normalizer().fit(x)\n",
    "x = transformer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot_encoding(categories ,labels):\n",
    "    num_samples = len(labels)\n",
    "    num_categories = len(categories)\n",
    "    one_hot = np.zeros((num_samples, num_categories))\n",
    "\n",
    "    # Perform one-hot encoding\n",
    "    for i, label in enumerate(labels):\n",
    "        index = categories.index(label)\n",
    "        one_hot[i, index] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded = oneHot_encoding([0,1,2,3,4,5,6,7,8,9],y)\n",
    "y_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(x.shape,42)\n",
    "model.addLayers_Dense(89, ANN.activation_tanh)\n",
    "model.addLayers_Dense(10, ANN.activation_sigmoid)\n",
    "model.compile(ANN.optimizer_gradientDescent, ANN.loss_MSE)\n",
    "model.fit(x, y_encoded, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.        , 0.        , 0.09024036, 0.23462493, 0.16243265,\n",
       "         0.01804807, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.23462493, 0.27072108, 0.18048072, 0.27072108, 0.09024036,\n",
       "         0.        , 0.        , 0.05414422, 0.27072108, 0.03609614,\n",
       "         0.        , 0.19852879, 0.14438458, 0.        , 0.        ,\n",
       "         0.07219229, 0.21657686, 0.        , 0.        , 0.14438458,\n",
       "         0.14438458, 0.        , 0.        , 0.09024036, 0.14438458,\n",
       "         0.        , 0.        , 0.16243265, 0.14438458, 0.        ,\n",
       "         0.        , 0.07219229, 0.19852879, 0.        , 0.01804807,\n",
       "         0.21657686, 0.1263365 , 0.        , 0.        , 0.03609614,\n",
       "         0.25267301, 0.09024036, 0.18048072, 0.21657686, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.10828843, 0.23462493,\n",
       "         0.18048072, 0.        , 0.        , 0.        ]]),\n",
       " array([[ 0.99988667, -1.        , -0.98022198, -0.99254163,  0.9905437 ,\n",
       "          0.95507904, -1.        , -1.        , -0.9999943 , -1.        ,\n",
       "          1.        ,  1.        , -0.99985631,  1.        ,  0.36931333,\n",
       "          0.99919564,  0.99999329,  0.9997736 ,  1.        , -0.99999992,\n",
       "          1.        , -0.99999995,  1.        , -1.        , -0.63761294,\n",
       "          1.        , -1.        ,  0.99849275, -0.9999968 , -0.99999989,\n",
       "         -1.        ,  1.        ,  0.9954043 ,  0.99707585, -0.49546636,\n",
       "         -0.99995881,  1.        , -1.        ,  0.22571605,  0.98703359,\n",
       "          0.99559515,  1.        , -0.99727249, -0.99368465,  1.        ,\n",
       "          0.97185134,  1.        ,  0.64047665, -1.        , -0.99996989,\n",
       "         -1.        ,  0.99988425,  1.        , -0.99999994,  1.        ,\n",
       "         -1.        ,  1.        , -1.        , -0.99930288, -1.        ,\n",
       "          1.        , -0.77838459, -0.99936343, -0.99999765, -0.97864595,\n",
       "          1.        , -1.        ,  0.99788386,  0.99999919, -0.9999175 ,\n",
       "          0.9977409 ,  0.99994   ,  0.99803709, -1.        ,  1.        ,\n",
       "          1.        ,  0.68239882, -0.99999918, -0.91174983, -1.        ,\n",
       "         -0.60446372, -1.        , -0.99999557, -0.58969132, -1.        ,\n",
       "         -1.        , -0.95600997,  0.99999738, -0.99985892]]),\n",
       " array([[1.00000000e+00, 2.79341503e-78, 1.00000000e+00, 1.00000000e+00,\n",
       "         1.00000000e+00, 1.00000000e+00, 9.28020906e-74, 2.39426326e-25,\n",
       "         5.70988989e-16, 5.77258843e-31]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.delta_l_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " array([[ 3.24585506e+02, -1.25915519e+03, -9.27198089e+02,\n",
       "          2.78172628e+02,  2.95292868e+02,  3.19558649e+02,\n",
       "         -5.10554928e+02,  6.18847606e+02, -5.47482647e+02,\n",
       "         -1.30137193e+03, -2.45390644e+02,  8.03143859e+01,\n",
       "         -6.41288412e+02, -5.16693802e+02, -7.24987319e+02,\n",
       "         -3.78239265e+02, -1.04368786e+03,  1.32802274e+02,\n",
       "         -2.81533937e+01, -4.32157379e+02,  1.69300272e+03,\n",
       "          5.98586577e+02,  6.69003997e+02, -1.07742385e+03,\n",
       "          7.07579592e+02, -8.77724172e+02,  1.07282964e+02,\n",
       "          1.51632140e+02,  3.95349027e+02, -2.08609761e+02,\n",
       "         -6.17028779e+02,  5.60655329e+02,  5.03497748e+02,\n",
       "          8.51292350e+02,  1.34021732e+03,  7.93485546e+02,\n",
       "         -4.89079953e+02,  6.44095574e+02, -3.67774430e+02,\n",
       "         -7.36707457e+02, -5.22669927e+02,  4.14709084e+02,\n",
       "          1.13076419e+03,  3.28044629e+02, -2.74891681e+02,\n",
       "         -9.22973356e+02, -1.08240653e+02,  1.85978136e+03,\n",
       "          4.04017992e+02,  5.36427465e+02, -8.96018351e+02,\n",
       "         -4.58597304e+02,  7.25795048e+01, -1.74059841e+00,\n",
       "         -6.97198254e+02, -1.11520139e+03, -1.72039891e+02,\n",
       "          8.75395499e+02,  6.57915709e+01, -7.10850828e+02,\n",
       "          1.40635907e+02,  5.50910610e+02, -6.93529584e+02,\n",
       "         -2.03956323e+02]]),\n",
       " array([[ -26.87779725,   -1.88382043,   64.44463202,  -25.944855  ,\n",
       "           26.25429814,   -0.47665259,  -44.60144808,   -3.98177124,\n",
       "          -47.30545088,  -50.22030387,  -73.73453527,   42.65297811,\n",
       "          -32.31630794,  -21.64607947,   15.60738548,   11.33191047,\n",
       "          -37.33995115,   27.80424544,   38.74584011,    7.17612467,\n",
       "           12.23310592,  -60.51646872,  -26.98720842,   25.36723211,\n",
       "         -113.7223707 ,   17.77316531,   14.23964272,   86.1417016 ,\n",
       "           41.02993077,   13.22584381,  -11.41055958,  -19.86792767,\n",
       "           -0.40583622,  -15.03102455,   -9.33466869,   -0.95638215,\n",
       "            5.848184  ,  -44.77691661,  -15.24327063,   44.2837165 ,\n",
       "           69.95344467,  -33.87743041,    7.95971183,   22.30207172,\n",
       "           17.49512443,  -11.34488328,  -12.46078166,  -25.35012288,\n",
       "           41.51139686,  -24.29074701,   31.98230572,  -45.9234933 ,\n",
       "           12.48429918,   31.27331   ,   68.64812281,  -25.37597891,\n",
       "           43.08471241,  -82.39460083,  -76.00208985,   62.18210711,\n",
       "          -35.49215293,  -30.04968982,    9.96351688,   12.18405452,\n",
       "           35.32278733,  -23.11299652,  -52.85889636,    0.66297804,\n",
       "          -11.72308187,   -3.17173679,    8.32650288,   39.53287549,\n",
       "           44.72167758,  -51.19662319,   -5.06461959,  -11.43763957,\n",
       "            2.66672118,   58.09013547,   23.75757268,  -56.54242182,\n",
       "          -21.62165037,  -38.27517161,    5.87442199,  -33.42426818,\n",
       "           -5.11352712,  -56.83595071,   12.70360043,   50.98178876,\n",
       "           22.44644927]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.delta_l_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN(x.shape,42)\n",
    "model.addLayers_Dense(89,ANN.activation_tanh)\n",
    "model.addLayers_Dense(10,ANN.activation_linear)\n",
    "model.addLayers_softmax()\n",
    "model.compile(ANN.optimizer_gradientDescent,ANN.loss_crossEntropy)\n",
    "model.fit(x,y_encoded,1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['None', 'None', 64],\n",
       " ['Dense', <function __main__.ANN.activation_tanh(z)>, 89],\n",
       " ['Dense', <function __main__.ANN.activation_linear(z)>, 10],\n",
       " ['Softmax', 10],\n",
       " ['Loss', <function __main__.ANN.loss_crossEntropy(P, y)>]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " array([[-2.21606798e-21, -2.67981488e-21,  3.53081971e-23,\n",
       "         -4.10005786e-21, -1.54493733e-21,  3.06351958e-21,\n",
       "         -3.01740705e-21, -5.53007366e-21, -4.02057844e-22,\n",
       "          2.10950172e-21, -3.81263736e-22,  3.01185702e-22,\n",
       "         -1.34481179e-21, -6.87468616e-21, -7.37657328e-23,\n",
       "         -6.06722538e-21,  5.05265764e-21,  2.69596834e-21,\n",
       "         -8.36417868e-21, -2.20745045e-21, -8.75116437e-21,\n",
       "          3.21212982e-21,  2.35789897e-21, -1.05175408e-21,\n",
       "         -2.19502474e-21,  3.68873994e-21,  4.79106415e-21,\n",
       "          3.04176731e-21, -2.64751057e-21,  1.98588536e-21,\n",
       "         -4.39546805e-21,  1.75588919e-21, -1.60507537e-20,\n",
       "          9.31840830e-21, -9.86725476e-21, -6.32877223e-22,\n",
       "         -3.49970596e-22, -4.10124358e-21, -5.30407210e-21,\n",
       "         -1.16216130e-20,  2.40897005e-22,  7.16325974e-22,\n",
       "          3.76883553e-21, -2.96202671e-22,  2.31425547e-21,\n",
       "         -2.66080333e-22, -5.28782773e-21,  5.44544791e-21,\n",
       "          1.48931527e-21, -1.04094749e-20, -5.31400088e-21,\n",
       "         -6.60918766e-21,  3.05206583e-21,  4.91944887e-21,\n",
       "         -5.07088343e-22,  4.20380343e-23,  1.13359066e-21,\n",
       "          5.66839939e-21,  1.92892863e-21, -2.80421107e-21,\n",
       "          6.26493224e-21,  1.15405309e-21, -3.73243386e-21,\n",
       "          3.65854395e-21]]),\n",
       " array([[ 5.05258381e-22, -6.71620276e-23, -9.39134248e-23,\n",
       "          4.58240154e-23,  8.94243912e-24,  3.75751331e-22,\n",
       "         -2.58672864e-22, -1.34771171e-22, -1.81505534e-22,\n",
       "          9.90561196e-23, -2.17005968e-22,  2.81346529e-22,\n",
       "         -7.54283440e-22, -4.87359540e-22,  4.33540685e-23,\n",
       "         -4.11180435e-22,  3.03303092e-22,  1.56756041e-22,\n",
       "          3.18375525e-22,  1.62090781e-22,  1.06462192e-23,\n",
       "         -3.16697060e-22, -4.12486804e-22,  5.05541685e-23,\n",
       "         -3.91911789e-22, -4.40932670e-22,  3.98763736e-22,\n",
       "          2.66809317e-22,  2.85432336e-22, -2.32937834e-22,\n",
       "         -4.02647178e-22,  2.27555399e-23, -2.23918432e-22,\n",
       "         -2.17733195e-22,  3.89717133e-22, -1.48588567e-22,\n",
       "         -9.67150920e-23, -2.38814713e-22, -4.00611597e-23,\n",
       "         -3.14812895e-22,  3.03616371e-22, -1.23971347e-22,\n",
       "          3.36070030e-23, -3.11302196e-22,  1.02462709e-22,\n",
       "          1.36642801e-23,  1.67089107e-22, -3.49944653e-22,\n",
       "          2.52017104e-22, -4.77432655e-23,  1.81790848e-22,\n",
       "         -2.95005536e-22,  2.22934769e-22, -1.37600922e-22,\n",
       "          1.61036063e-22, -1.15712649e-22,  3.31085722e-22,\n",
       "         -4.01901708e-22, -5.98707948e-22,  1.67806705e-23,\n",
       "         -3.69980734e-22, -3.34435372e-22,  7.97205396e-23,\n",
       "          3.83937187e-22,  3.90431324e-22, -7.31423921e-23,\n",
       "         -1.19994631e-22,  2.59093932e-22, -1.25644088e-22,\n",
       "         -2.36855236e-22, -3.66013009e-22,  3.07458465e-22,\n",
       "          1.27836775e-22,  3.75452825e-23, -1.07535161e-23,\n",
       "          1.81040713e-22, -2.12767484e-22,  2.89303351e-22,\n",
       "         -5.03185234e-23, -2.62188114e-22, -6.76461798e-23,\n",
       "         -1.58120125e-22,  4.88546325e-23,  6.15340359e-22,\n",
       "          3.78618684e-22, -3.41347900e-22, -2.43646304e-22,\n",
       "          3.99617889e-22, -5.59186407e-23]]),\n",
       " array([[0.00000000e+000, 4.61192335e-157, 2.60141345e-023,\n",
       "         3.05891362e-039, 3.99676984e-040, 2.68193887e-038,\n",
       "         2.91892550e-145, 2.64784879e-098, 5.37373383e-087,\n",
       "         1.20042611e-096]])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.delta_l_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_331",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
